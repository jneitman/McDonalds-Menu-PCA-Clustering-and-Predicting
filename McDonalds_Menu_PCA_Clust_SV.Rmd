---
title: "McDonalds Menu - PCA, Clustering, and SVC/SVM"
author: Joel Neitman
date: September 9, 2005
output: 
  html_document:
    keep_md: true
---



Load data and exploratory analysis
```{r}
menu = read.csv("menu.csv")
library(treemap)
treemap(menu, index = c("Category","Item"), vSize = "Calories", type = "index", fontsize.labels = c(12.5,4.4), fontcolor.labels = "black", title = "McDonald's Menu: Calories per Item")
summary(menu)
#Breakfast = 1 : 42
#Beef & Pork =  43 : 57
#Chick & Fish = 58 : 84
#Salads = 85 : 90
#Snacks & Sides =  91 : 103
#Desserts = 104 : 110
#Beverages = 111 : 137
#Coffee & Tea = 138 : 232
#Smoothies & Shakes = 233 : 260

```

Remove redundant variables and cast more sensible names
```{r}
menu_daily = menu[,-c(1,2,3,5,6,8,11,13,15,17)] #remove Categories, Item, Serving Size, Calories.from.Fat, and non-Daily% variables.
str(menu_daily)
colnames(menu_daily) = c("Calories", "Total.Fat", "Sat.Fat", "Trans.Fat", "Chol", "Sodium", "Carbs", "Fiber", "Sugar", "Protein", "Vit.A", "Vit.C", "Calcium", "Iron")
```

```{r}
pairs(menu_daily[1:7])
pairs(menu_daily[8:14])
pairs(menu_daily[1:2])
boxplot(scale(menu_daily), las = 2, cex.axis = .8, frame = F, main = "Nutritional Value of McDonald's Menu", cex.main = 1)
axis(1, col = "white", labels = F, tck = F)
title(sub = "(Scaled)", cex.sub = .8)
summary(menu_daily)
```

```{r}
#remove only two outliers outliers
menu_daily = menu_daily[-83,]
menu_daily = menu_daily[-48,]

#remove same outliers from full dataset
menu = menu[-83,]
menu = menu[-48,]
```


Principle Component Analysis
```{r}
pca.menu = prcomp(menu_daily, scale = T) #calculate PCs

library(plotrix)
library(ggplot2)

#skree plots
pr_var = pca.menu$sdev^2
prop_varex = pr_var/sum(pr_var)
plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")

plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")

#biplot
library(ggfortify)
autoplot(pca.menu, data = menu,  colour = "Category",
         loadings = TRUE, loadings.colour = alpha('blue',.5),
         loadings.label = TRUE, loadings.label.size = 2.8, loadings.label.colour = "black") +
  theme(
    panel.background = element_blank(),
    legend.key = element_blank()
  ) +
  scale_color_manual(values = c("burlywood4", "cornflowerblue", "darkgoldenrod1", "brown3", "darkkhaki", "deeppink3", "forestgreen", "indianred1", "springgreen3"))
```


```{r}
menu_daily_scaled = scale(menu_daily) #standardize variables
```

Hierarchical Clustering
```{r}
#Hierarchical clustering using euclidean distance and ward.D linkage

menu_dist = dist(menu_daily_scaled, method = "euclidean") #obtain euclidean distance
hc.complete.x = hclust(menu_dist, method = "ward.D") #form h.clusters with ward.D linkage

plot(hc.complete.x, cex = 0.5, labels = F, main = "Dendrogram Using ward.D Linkage", xlab = "Menu Items") #plot h.clusters
abline(h = 28, col = "red", lty = 2) #9 clusters
abline(h = 75, col = "darkblue", lty = 2) #4 clusters
legend("topright", legend = c("4 Clusters", "9 Clusters"), col = c("darkblue", "red"), lty = c(2,2), box.lty = 0, cex = 0.8)

```

```{r}
##### 9 clusters #####
x.colors = c("orange", "deeppink", "blue", "red", "seagreen", "blueviolet", "turquoise", "firebrick", "chartreuse3") #list of colors to outline clusters

nclust = 9
x.cut = cutree(hc.complete.x, k = nclust) #"Cut tree" to 9 clusters

#Daily Total Fat vs Calories
plot(menu_daily$Calories, menu_daily$Total.Fat, pch = 16, main = paste(nclust," Clusters Joined by ward.D Linkage"), xlab = "Calories", ylab = "Daily Value of Total Fat (%)", frame.plot = F)
for (i in 1:nclust)  points(menu_daily$Calories[x.cut == i], menu_daily$Total.Fat[x.cut == i], pch = 16, col = x.colors[i])
```

```{r}
#Table showing contents of 9 hierarchical clusters.
library(formattable)

cluster9_df = as.data.frame.matrix(table(menu$Category, x.cut))

formattable(cluster9_df, list("1" = color_tile("snow","orange"), "2" = color_tile("snow","deeppink"), "3" = color_tile("snow","blue"), "4" = color_tile("snow","red"), "5" = color_tile("snow", "seagreen"), "6" = color_tile("snow", "blueviolet"), "7" = color_tile("snow", "turquoise"), "8" = color_tile("white", "firebrick"), "9" = color_tile("snow", "chartreuse3")))

#Ill-defined clusters with some being composed of very few data points.  Each category seems NOT to create its own cluster.  Reduce the number of clusters.

```

```{r}
##### 4 clusters #####
nclust2 = 4
x.cut2 = cutree(hc.complete.x, k = nclust2) #"Cut tree" to 4 clusters

par(mfrow = c(2,2))
#Daily Total Fat vs Calories
plot(menu_daily$Calories, menu_daily$Total.Fat, pch = 16 ,main = "Total Fat vs Calories", xlab = "Calories", ylab = "Daily Total Fat (%)")
for (i in 1:nclust2)  points(menu_daily$Calories[x.cut2 == i], menu_daily$Total.Fat[x.cut2 == i], pch = 16, col = x.colors[i])

#Daily Sodium vs Calories
plot(menu_daily$Carbs, menu_daily$Chol, pch = 16 ,main = "Cholesterol vs Carbohydrates", xlab = "Daily Carbohydrates (%)", ylab = "Daily Cholesterol (%)")
for (i in 1:nclust2)  points(menu_daily$Carbs[x.cut2 == i], menu_daily$Chol[x.cut2 == i], pch = 16, col = x.colors[i])

#Fiber vs Calories
plot(menu_daily$Fiber, menu_daily$Sodium, pch = 16 ,main = "Sodium vs Fiber", xlab = " Daily Dietary Fiber (%)", ylab = "Daily Sodium (%)")
for (i in 1:nclust2)  points(menu_daily$Fiber[x.cut2 == i], menu_daily$Sodium[x.cut2 == i], pch = 16, col = x.colors[i])

#Protein vs Calories
plot(menu_daily$Protein, menu_daily$Sugar, pch = 16 ,main = "Sugar vs Protein", xlab = "Protein", ylab = "Sugar")
for (i in 1:nclust2)  points(menu_daily$Protein[x.cut2 == i], menu_daily$Sugar[x.cut2 == i], pch = 16, col = x.colors[i])
```

```{r}
#Table showing contents of 4 hierarchical clusters.
cluster5_df = as.data.frame.matrix(table(menu$Category, x.cut2))

formattable(cluster5_df, list("1" = color_tile("snow","orange"), "2" = color_tile("snow","deeppink"), "3" = color_tile("snow","blue"), "4" = color_tile("snow","red")))
```



K-means Clustering
```{r}
#K-mean clustering
kclust = 4 #4 k-mean clusters

set.seed(100)
km.x.4 = kmeans(menu_daily_scaled, kclust)$cluster

par(mfrow = c(2,2))
plot(menu_daily$Calories, menu_daily$Total.Fat, pch=16 ,main = "Total Fat vs Calories", xlab = "Calories", ylab = "Daily Total Fat (%)")
for (i in 1:kclust)  points(menu_daily$Calories[km.x.4 == i], menu_daily$Total.Fat[km.x.4 == i], pch=16, col=x.colors[i])

plot(menu_daily$Carbs, menu_daily$Chol, pch=16 ,main = "Cholesterol vs Carbohydrates", xlab = "Daily Carbohydrates (%)", ylab = "Daily Cholesterol (%)")
for (i in 1:kclust)  points(menu_daily$Carbs[km.x.4 == i], menu_daily$Chol[km.x.4 == i], pch=16, col=x.colors[i])

plot(menu_daily$Fiber, menu_daily$Sodium, pch=16 ,main = "Sodium vs Fiber", xlab = "Daily Dietary Fiber (%)", ylab = "Daily Sodium (%)")
for (i in 1:kclust)  points(menu_daily$Fiber[km.x.4 == i], menu_daily$Sodium[km.x.4 == i], pch=16, col=x.colors[i])

plot(menu_daily$Protein, menu_daily$Sugar, pch=16 ,main = "Sugar vs Protein", xlab = "Protein", ylab = "Sugar")
for (i in 1:kclust)  points(menu_daily$Protein[km.x.4 == i], menu_daily$Sugar[km.x.4 == i], pch=16, col=x.colors[i])

```

```{r}
#Table showing contents of k-mean clusters
kcluster4 = as.data.frame.matrix(table(menu$Category, km.x.4))

formattable(kcluster4, list("1" = color_tile("snow","orange"), "2" = color_tile("snow","deeppink"), "3" = color_tile("snow","blue"), "4" = color_tile("snow","red")))
```



SVC and SVM
```{r}
##### Start of Support Vector Classifer and Machine #####
library(e1071)
menu_svm = menu[,-c(2,3,5,6,8,11,13,15,17)] #remove Serving Size, Item, Calories.from.Fat, and non-daily% variables.

#Correlation Matrix (HEAT MAP)
menu_cor = cor(cbind(Calcium = menu_svm$Calories, Tot.Fat = menu_svm$Total.Fat....Daily.Value., S.Fat = menu_svm$Saturated.Fat....Daily.Value., T.Fat = menu_svm$Trans.Fat, Chole. = menu_svm$Cholesterol....Daily.Value., Sodium = menu_svm$Sodium....Daily.Value., Carb. = menu_svm$Carbohydrates....Daily.Value., Fiber = menu_svm$Dietary.Fiber....Daily.Value., Sugar = menu_svm$Sugars, Protein = menu_svm$Protein, Vit.A = menu_svm$Vitamin.A....Daily.Value., Vit.C = menu_svm$Vitamin.C....Daily.Value., Calcium = menu_svm$Calcium....Daily.Value., Iron = menu_svm$Iron....Daily.Value.))

image(x=seq(dim(menu_cor)[2]), y=seq(dim(menu_cor)[2]), z=menu_cor, xlab = "", ylab = "", axes = F, main = "Correlation Matrix")
text(expand.grid(x=seq(dim(menu_cor)[2]), y=seq(dim(menu_cor)[2])), labels=round(c(menu_cor),2))
axis(1, at=seq(nrow(menu_cor)), labels = rownames(menu_cor), las=2)
axis(2, at=seq(ncol(menu_cor)), labels = colnames(menu_cor), las=1, cex.axis = .8)
 

#Multicolinearity present among almost all the variables.
```

```{r}
#Assessing normality and variances
menu_daily_scaled = as.data.frame(menu_daily_scaled)

boxplot(menu_daily_scaled, las = 2, names = c("Calories", "Total.Fat (%DV)", "Sat.Fat (%DV)", "Trans.Fat (%DV)", "Chole. (%DV)", "Sodium (%DV)", "Carbs. (%DV)", "Fiber (%DV)", "Sugar", "Protein", "Vit.A (%DV)", "Vit.C (%DV)", "Calcium (%DV)", "Iron (%DV)"), cex.axis = .6)

#Most variables have a slight to heavy right skew and unequal variances.  Methods like liear discriminant analysis and others with similar assumptions would not be appropriate.  
```

```{r}
################################# MODEL SELECTION ################################# 
    fulldata.in = menu_svm
    k.in = 10
    n.in = dim(fulldata.in)[1]
    groups.in = c(rep(1:k.in,floor(n.in/k.in)),1:(n.in%%k.in))
    
    set.seed(101)
    cvgroups.in = sample(groups.in,n.in) 
    
    #support vector classifier
    menu.tune.linear = tune(svm, Category ~ ., data = fulldata.in[-cvgroups.in,], kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)), 
      type = "C-classification")
    
    #support vector machine
    menu.tune.radial = tune(svm, Category ~ ., data = fulldata.in[-cvgroups.in,], kernel = "radial", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100), 
      gamma = c(0.5, 1, 2, 3, 4)), type = "C-classification")
    
    best_selection = c(menu.tune.linear$best.performance, menu.tune.radial$best.performance)
    if(which.min(best_selection) == 1){
      print("menu.tune.linear")
    } else{
      print("menu.tune.radial")
    }

```

```{r}
################################# DOUBLE CROSS VALIDATION ################################# 
fulldata.out = menu_svm
k.out = 10 
n.out = dim(fulldata.out)[1]
groups.out = c(rep(1:k.out,floor(n.out/k.out)),1:(n.out%%k.out))  #produces list of group labels
  
set.seed(102)
cvgroups.out = sample(groups.out,n.out)  #orders randomly, with seed (8)

allpredictedCV.out = rep(NA,n.out)
for (j in 1:k.out)  { 
  groupj.out = (cvgroups.out == j)
  traindata.out = menu_svm[!groupj.out,]
  testdata.out = menu_svm[groupj.out,]
  
  ###Model-fitting process###
    fulldata.in = traindata.out
    k.in = 10
    n.in = dim(fulldata.in)[1]
    groups.in = c(rep(1:k.in,floor(n.in/k.in)),1:(n.in%%k.in))
    cvgroups.in = sample(groups.in,n.in) 
    
    #support vector classifier
    menu.tune.linear = tune(svm, Category ~ ., data = fulldata.in[-cvgroups.in,], kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)), 
      type = "C-classification")
    
    #support vector machine
    menu.tune.radial = tune(svm, Category ~ ., data = fulldata.in[-cvgroups.in,], kernel = "radial", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100), 
      gamma = c(0.5, 1, 2, 3, 4)), type = "C-classification")
    
  ###Use the model with the best performance (lowest CV)###
    if(menu.tune.linear$best.performance < menu.tune.radial$best.performance){
      print(paste("linear:", "cost = ", menu.tune.linear$best.parameters));
      allpredictedCV.out[groupj.out] = predict(menu.tune.linear$best.model, testdata.out)
    } else{
      print(paste("radial:","cost = ", menu.tune.radial$best.parameters[,1], ",", "gamma = ", menu.tune.radial$best.parameters[,2]));
      allpredictedCV.out[groupj.out] = predict(menu.tune.radial$best.model, testdata.out)
      }
}

#Final assessment of predictions
y.out = fulldata.out$Category

###Convert the predictions (numerical) to category names
#unique(cbind(y.out)) #order of numbers is the same as the order for names
#unique(y.out)
allpredictedCV.out[allpredictedCV.out == 3] = "Breakfast"
allpredictedCV.out[allpredictedCV.out == 1] = "Beef & Pork"
allpredictedCV.out[allpredictedCV.out == 4] = "Chicken & Fish"
allpredictedCV.out[allpredictedCV.out == 7] = "Salads"
allpredictedCV.out[allpredictedCV.out == 9] = "Snacks & Sides"
allpredictedCV.out[allpredictedCV.out == 6] = "Desserts"
allpredictedCV.out[allpredictedCV.out == 2] = "Beverages"
allpredictedCV.out[allpredictedCV.out == 5] = "Coffee & Tea"
allpredictedCV.out[allpredictedCV.out == 8] = "Smoothies & Shakes"
CV.out = sum(allpredictedCV.out != y.out)/n.out
print(paste("1-CV = ", 1-CV.out))

```

```{r}
#Fit the best model to the full data set (with tune function)
set.seed(100)
menu_best_smv = tune(svm, Category ~ ., data = menu_svm, kernel = "linear", ranges = list(cost = c(.001, .01, .1, 1, 5, 10, 100)), 
      type = "C-classification")
best_model = menu_best_smv$best.model
best_model
#112 support vectors
percent_match = 1 - sum(menu_best_smv$best.model$fitted != menu_svm$Category)/length(menu_svm$Category)
print(paste("% Matched =", percent_match))
```

```{r}
svm.table = as.data.frame.matrix(table(best_model$fitted, menu_svm$Category))
formattable(svm.table, list("Beef & Pork" = color_tile("snow","darkorange2"), "Beverages" = color_tile("snow", "mediumseagreen"), "Breakfast" = color_tile("snow", "orchid"), "Chicken & Fish" = color_tile("snow", "yellow3"), "Coffee & Tea" = color_tile("snow", "turquoise3"), "Desserts" = color_tile("snow", "hotpink"), "Salads" = color_tile("snow", "limegreen"), "Smoothies & Shakes" = color_tile("snow", "steelblue1"), "Snacks & Sides" = color_tile("snow", "salmon")))

```

```{r}
#See if new products identify as their assigned category (assignment by McDonald's); outliers previously removed will be tested as an example
menu_raw = read.csv("menu.csv")
menu_raw[c(48,83), c(1:2)]
new_data = menu_raw[c(48,83),-c(1,2,3,5,6,8,11,13,15,17)]

```

```{r}
#predict outliers  
predict(best_model, newdata = new_data)
#The model successfully predicted the outlier categories.
```

